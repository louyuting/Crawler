1.  你先从简单的写起，随便选一个网站作为目标，然后模拟http请求，httpclient可以，然后把爬下来的东西存起来，
思路就是这个思路，但是可以慢慢扩展.


2.  比方说一开始用单线程爬，后面用多线程爬，再后面可以用集群爬。


3.  数据存储，一开始可以用文本存，后续可以用数据库，如果你爬的网站有明确的格式，那可以在爬的时候就做解析，
然后放到MySQL之类的关系数据库里，要是没有明确的格式，可以放到mongodb里，后续再处理


4.  然后网址去重，就是已经爬过的内容不能再爬，你可以用数据库来帮你去重，也可以用布隆过滤器之类的东西来做


5.  还有反爬虫的东西，这个也很多，你可以慢慢做，慢慢加功能，最后对爬下来的数据做个分析，就是一个完整的系统了，
开源到github上。

